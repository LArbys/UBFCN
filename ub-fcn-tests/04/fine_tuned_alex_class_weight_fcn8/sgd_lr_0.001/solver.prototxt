train_net: "train.prototxt"
display: 1

#vic
#average_loss: 20

lr_policy: "fixed"
# lr for normalized softmax
base_lr: 0.001
# standard momentum
momentum: 0.9


# gradient accumulation (https://groups.google.com/forum/#!topic/caffe-users/PMbycfbpKcY)
# A weight update / iteration is done for batch_size * iter_size inputs at a time. 
# Each solver iteration reported is accumulated by running `iter_size` calls to forward + backward. See
iter_size: 1

max_iter: 100000
weight_decay: 0.0005
snapshot: 4000
snapshot_prefix: "fcn_class_weight"
#test_initialization: false
#debug_info: true